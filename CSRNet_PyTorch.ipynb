{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzdH90Zy7z7U",
        "outputId": "1a9780aa-dc1c-4097-ba79-626881df47b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/tthien/shanghaitech?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 333M/333M [00:17<00:00, 19.5MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/tthien/shanghaitech/versions/1\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download Dataset\n",
        "path = kagglehub.dataset_download(\"tthien/shanghaitech\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "36PgSmN177db"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from scipy.io import loadmat\n",
        "import numpy as np\n",
        "import cv2\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import scipy.ndimage\n",
        "\n",
        "class ShanghaiTechDataset(Dataset):\n",
        "    def __init__(self, root_dir, part, transform=None, gt_downsample=8, sigma=5):\n",
        "        self.root_dir = os.path.join(root_dir, f'part_{part}', 'train_data')\n",
        "        self.image_dir = os.path.join(self.root_dir, 'images')\n",
        "        self.density_dir = os.path.join(self.root_dir, 'ground-truth')\n",
        "        self.transform = transform\n",
        "        self.gt_downsample = gt_downsample\n",
        "        self.sigma = sigma\n",
        "        self.image_files = [f for f in os.listdir(self.image_dir) if f.endswith('.jpg')]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Loading the image\n",
        "        img_path = os.path.join(self.image_dir, self.image_files[idx])\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        # Loading .mat file and extracting coordinates\n",
        "        mat_path = os.path.join(self.density_dir, f'GT_{self.image_files[idx].replace(\".jpg\", \".mat\")}')\n",
        "        mat = loadmat(mat_path)\n",
        "        points = mat['image_info'][0][0][0][0][0]\n",
        "\n",
        "        # Generating density map with original image size\n",
        "        density_map = self.generate_density_map(image.size, points)\n",
        "\n",
        "        # Downsampling density map to match model output size (1/8 of image size)\n",
        "        density_map = cv2.resize(density_map, (image.size[0] // self.gt_downsample, image.size[1] // self.gt_downsample))\n",
        "        density_map = density_map[np.newaxis, :, :] * (self.gt_downsample ** 2)  # Scale to keep total count the same\n",
        "\n",
        "        # Applying image transformations\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Converting density map to torch tensor\n",
        "        density_map = torch.from_numpy(density_map).float()\n",
        "\n",
        "        return image, density_map\n",
        "\n",
        "    def generate_density_map(self, image_shape, points):\n",
        "        \"\"\"\n",
        "        Generates a density map for an image based on provided points.\n",
        "        Each point is represented as a Gaussian in the density map.\n",
        "        \"\"\"\n",
        "        density_map = np.zeros((image_shape[1], image_shape[0]), dtype=np.float32)\n",
        "\n",
        "        for point in points:\n",
        "            x, y = min(int(point[0]), image_shape[0] - 1), min(int(point[1]), image_shape[1] - 1)\n",
        "            density_map[y, x] += 1\n",
        "\n",
        "        # Applying Gaussian blur\n",
        "        density_map = cv2.GaussianBlur(density_map, (self.sigma, self.sigma), self.sigma)\n",
        "        return density_map\n",
        "\n",
        "\n",
        "# Data augmentation and transformation\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Loading train dataset\n",
        "dataset_A = ShanghaiTechDataset(\n",
        "    root_dir='/root/.cache/kagglehub/datasets/tthien/shanghaitech/versions/1/ShanghaiTech',\n",
        "    part='A',\n",
        "    transform=transform\n",
        ")\n",
        "dataset_B = ShanghaiTechDataset(\n",
        "    root_dir='/root/.cache/kagglehub/datasets/tthien/shanghaitech/versions/1/ShanghaiTech',\n",
        "    part='B',\n",
        "    transform=transform\n",
        ")\n",
        "train_dataloader_A = DataLoader(dataset_A, batch_size=1, shuffle=True)\n",
        "train_dataloader_B = DataLoader(dataset_B, batch_size=1, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PMH2MBjl8W06"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "# Model Architecture\n",
        "class CSRNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CSRNet, self).__init__()\n",
        "\n",
        "        # Frontend: VGG-16 layers (up to conv4_3 layer)\n",
        "        self.frontend = models.vgg16(pretrained=True).features[:23]\n",
        "\n",
        "        # Backend: Dilated convolutional layers\n",
        "        self.backend = nn.Sequential(\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=2, dilation=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=2, dilation=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=2, dilation=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(512, 256, kernel_size=3, padding=2, dilation=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(256, 128, kernel_size=3, padding=2, dilation=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 64, kernel_size=3, padding=2, dilation=2),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        # Output layer for density map generation\n",
        "        self.output_layer = nn.Conv2d(64, 1, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.frontend(x)\n",
        "        x = self.backend(x)\n",
        "        x = self.output_layer(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = CSRNet().cuda()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OII-kDSzQ-Pb",
        "outputId": "c2250728-6900-4761-eb57-944cf364bb35"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
            "100%|██████████| 528M/528M [00:06<00:00, 81.9MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8MAeUIqpRDif",
        "outputId": "dd76b1eb-0d51-46f0-b76c-a899fd4cd09e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CSRNet(\n",
              "  (frontend): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (6): ReLU(inplace=True)\n",
              "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (8): ReLU(inplace=True)\n",
              "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (13): ReLU(inplace=True)\n",
              "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (15): ReLU(inplace=True)\n",
              "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (18): ReLU(inplace=True)\n",
              "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (20): ReLU(inplace=True)\n",
              "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (22): ReLU(inplace=True)\n",
              "  )\n",
              "  (backend): Sequential(\n",
              "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n",
              "    (1): ReLU()\n",
              "    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n",
              "    (3): ReLU()\n",
              "    (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n",
              "    (5): ReLU()\n",
              "    (6): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n",
              "    (7): ReLU()\n",
              "    (8): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n",
              "    (9): ReLU()\n",
              "    (10): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n",
              "    (11): ReLU()\n",
              "  )\n",
              "  (output_layer): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-QiAWUp8bHz",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "import torch\n",
        "\n",
        "\n",
        "# Defining Model, Loss Function, and Optimizer\n",
        "model = CSRNet().cuda() if torch.cuda.is_available() else CSRNet()\n",
        "criterion = torch.nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
        "\n",
        "train_loss_history = []\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for i, (images, density_maps) in enumerate(train_dataloader_B):\n",
        "        images = images.cuda() if torch.cuda.is_available() else images\n",
        "        density_maps = density_maps.cuda() if torch.cuda.is_available() else density_maps\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, density_maps)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if (i + 1) % 10 == 0:\n",
        "            print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_dataloader_B)}], Loss: {loss.item():.4f}')\n",
        "\n",
        "    epoch_loss = running_loss / len(train_dataloader_B)\n",
        "    train_loss_history.append(epoch_loss)\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cpZZ5R_I8dAz"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plotting Training Loss over Epochs\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(1, num_epochs + 1), train_loss_history, label='Training Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss Over Epochs')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def mae(pred, gt):\n",
        "    return np.abs(pred - gt).sum()\n",
        "\n",
        "def mse(pred, gt):\n",
        "    return ((pred - gt) ** 2).sum()"
      ],
      "metadata": {
        "id": "MrtDUGReAkM_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ShanghaiTechTestDataset(Dataset):\n",
        "    def __init__(self, root_dir, part, transform=None, gt_downsample=8, sigma=5):\n",
        "        self.root_dir = os.path.join(root_dir, f'part_{part}', 'test_data')\n",
        "        self.image_dir = os.path.join(self.root_dir, 'images')\n",
        "        self.density_dir = os.path.join(self.root_dir, 'ground-truth')\n",
        "        self.transform = transform\n",
        "        self.gt_downsample = gt_downsample\n",
        "        self.sigma = sigma\n",
        "        self.image_files = [f for f in os.listdir(self.image_dir) if f.endswith('.jpg')]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Loading the image\n",
        "        img_path = os.path.join(self.image_dir, self.image_files[idx])\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        # Loading .mat file and extract coordinates\n",
        "        mat_path = os.path.join(self.density_dir, f'GT_{self.image_files[idx].replace(\".jpg\", \".mat\")}')\n",
        "        mat = loadmat(mat_path)\n",
        "        points = mat['image_info'][0][0][0][0][0]  # Coordinates of people\n",
        "\n",
        "        # Generating density map with original image size\n",
        "        density_map = self.generate_density_map(image.size, points)\n",
        "\n",
        "        # Downsampling density map to match model output size (1/8 of image size)\n",
        "        density_map = cv2.resize(density_map, (image.size[0] // self.gt_downsample, image.size[1] // self.gt_downsample))\n",
        "        density_map = density_map[np.newaxis, :, :] * (self.gt_downsample ** 2)  # Scale to keep total count the same\n",
        "\n",
        "        # Applying image transformations if provided\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Converting density map to torch tensor\n",
        "        density_map = torch.from_numpy(density_map).float()\n",
        "\n",
        "        return image, density_map\n",
        "\n",
        "    def generate_density_map(self, image_shape, points):\n",
        "        \"\"\"\n",
        "        Generates a density map for an image based on provided points.\n",
        "        Each point is represented as a Gaussian in the density map.\n",
        "        \"\"\"\n",
        "        density_map = np.zeros((image_shape[1], image_shape[0]), dtype=np.float32)\n",
        "\n",
        "        for point in points:\n",
        "            x, y = min(int(point[0]), image_shape[0] - 1), min(int(point[1]), image_shape[1] - 1)\n",
        "            density_map[y, x] += 1\n",
        "\n",
        "        # Applying Gaussian blur\n",
        "        density_map = cv2.GaussianBlur(density_map, (self.sigma, self.sigma), self.sigma)\n",
        "        return density_map\n",
        "\n",
        "\n",
        "# Data augmentation and transformation\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Loading test dataset\n",
        "test_dataset_A = ShanghaiTechTestDataset(\n",
        "    root_dir='/root/.cache/kagglehub/datasets/tthien/shanghaitech/versions/1/ShanghaiTech',\n",
        "    part='A',\n",
        "    transform=transform\n",
        ")\n",
        "test_dataset_B = ShanghaiTechTestDataset(\n",
        "    root_dir='/root/.cache/kagglehub/datasets/tthien/shanghaitech/versions/1/ShanghaiTech',\n",
        "    part='B',\n",
        "    transform=transform\n",
        ")\n",
        "test_dataloader_A = DataLoader(test_dataset_A, batch_size=1, shuffle=False)\n",
        "test_dataloader_B = DataLoader(test_dataset_B, batch_size=1, shuffle=False)"
      ],
      "metadata": {
        "id": "zvRoXHEjUe80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "ebf3d1ef-a68a-462e-9c2e-0c5905975ba4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'Dataset' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-6e1daf3632e5>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mShanghaiTechTestDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_downsample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'part_{part}'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test_data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'images'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdensity_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ground-truth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Dataset' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "total_mae, total_mse = 0, 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, density_maps in test_dataloader_B:\n",
        "        images = images.cuda() if torch.cuda.is_available() else images\n",
        "        density_maps = density_maps.cuda() if torch.cuda.is_available() else density_maps\n",
        "\n",
        "        # Prediction\n",
        "        outputs = model(images)\n",
        "\n",
        "        # Computing predicted count by summing the density map values\n",
        "        pred_count = outputs.squeeze().cpu().numpy().sum()\n",
        "\n",
        "        # Computing ground-truth count by summing the density map values\n",
        "        gt_count = density_maps.squeeze().cpu().numpy().sum()\n",
        "\n",
        "        # Calculating MAE and MSE for this sample\n",
        "        total_mae += mae(pred_count, gt_count)\n",
        "        total_mse += mse(pred_count, gt_count)\n",
        "\n",
        "# Calculating the average MAE and MSE over the entire test set\n",
        "avg_mae = total_mae / len(test_dataloader_B)\n",
        "avg_mse = np.sqrt(total_mse / len(test_dataloader_B))\n",
        "\n",
        "print(f\"Mean Absolute Error (MAE): {avg_mae:.2f}\")\n",
        "print(f\"Mean Squared Error (MSE): {avg_mse:.2f}\")\n"
      ],
      "metadata": {
        "id": "MXt9G8G8AnAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model weights\n",
        "torch.save(model.state_dict(), 'csrnet_model.pth')\n",
        "\n",
        "# Save the entire model\n",
        "torch.save(model, 'csrnet_full_model.pth')\n"
      ],
      "metadata": {
        "id": "JX9Rhu1LAodK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bXXNYglrwXmy"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}